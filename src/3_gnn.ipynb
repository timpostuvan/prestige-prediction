{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "from exploration.dataset import PyGAcademicGraph\n",
    "\n",
    "from utils import train, evaluate\n",
    "\n",
    "from exploitation.models import GAT, MPGCN_Net, GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples in the train dataset:  9\n",
      "Number of samples in the val dataset:  9\n",
      "Number of samples in the test dataset:  9\n",
      "Output of one sample from the train dataset:  Data(edge_index=[2, 36936], y=[359], x=[359, 8], edge_attr=[36936, 3], domain='Academia', train_mask=[359], val_mask=[359], test_mask=[359], mask=[359])\n",
      "Edge_index :\n",
      "tensor([[  0,   0,   0,  ..., 357, 358, 358],\n",
      "        [  1,  89, 121,  ..., 352,  89, 358]])\n",
      "Number of features per node:  8\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1\n",
    "threshold = None\n",
    "\n",
    "# train dataset\n",
    "train_dataset = PyGAcademicGraph(split=\"train\", setting=\"transductive\", sparcify_threshold=threshold)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "\n",
    "# val dataset\n",
    "val_dataset = PyGAcademicGraph(split=\"val\", setting=\"transductive\", sparcify_threshold=threshold)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "# test dataset\n",
    "test_dataset = PyGAcademicGraph(split=\"test\", setting=\"transductive\", sparcify_threshold=threshold)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "# number of features\n",
    "n_features = train_dataset[0].x.shape[1]\n",
    "\n",
    "print(\"Number of samples in the train dataset: \", len(train_dataset))\n",
    "print(\"Number of samples in the val dataset: \", len(test_dataset))\n",
    "print(\"Number of samples in the test dataset: \", len(test_dataset))\n",
    "print(\"Output of one sample from the train dataset: \", train_dataset[0])\n",
    "print(\"Edge_index :\")\n",
    "print(train_dataset[0].edge_index)\n",
    "print(\"Number of features per node: \", n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Device:  cuda\n",
      "Epoch 00001 | Loss: 0.3448\n",
      "MSE: 0.3361\n",
      "Epoch 00002 | Loss: 0.3448\n",
      "Epoch 00003 | Loss: 0.3448\n",
      "Epoch 00004 | Loss: 0.3448\n",
      "Epoch 00005 | Loss: 0.3448\n",
      "Epoch 00006 | Loss: 0.3448\n",
      "MSE: 0.3361\n",
      "Epoch 00007 | Loss: 0.3448\n",
      "Epoch 00008 | Loss: 0.3448\n",
      "Epoch 00009 | Loss: 0.3448\n",
      "Epoch 00010 | Loss: 0.3448\n",
      "Epoch 00011 | Loss: 0.3448\n",
      "MSE: 0.3361\n",
      "Epoch 00012 | Loss: 0.3448\n",
      "Epoch 00013 | Loss: 0.3448\n",
      "Epoch 00014 | Loss: 0.3448\n",
      "Epoch 00015 | Loss: 0.3448\n",
      "Epoch 00016 | Loss: 0.3448\n",
      "MSE: 0.3361\n",
      "Epoch 00017 | Loss: 0.3448\n",
      "Epoch 00018 | Loss: 0.3448\n",
      "Epoch 00019 | Loss: 0.3448\n",
      "Epoch 00020 | Loss: 0.3448\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"\\nDevice: \", device)\n",
    "\n",
    "\n",
    "num_epochs = 20\n",
    "graph_convolution_no_weights = MPGCN_Net(\n",
    "    in_channels=n_features,\n",
    "    hidden_channels=64,\n",
    "    out_channels=1,\n",
    "    ).to(device)\n",
    "\n",
    "loss_fcn = nn.MSELoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(graph_convolution_no_weights.parameters(), lr=0.005)\n",
    "\n",
    "epoch_list, GCN_MSE = train(graph_convolution_no_weights, loss_fcn, device, optimizer, num_epochs, train_dataloader, val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Device:  cuda\n",
      "Epoch 00001 | Loss: 43187.3887\n",
      "MSE: 36131.2617\n",
      "Epoch 00002 | Loss: 30311.8243\n",
      "Epoch 00003 | Loss: 19599.6699\n",
      "Epoch 00004 | Loss: 14692.8077\n",
      "Epoch 00005 | Loss: 10291.0937\n",
      "Epoch 00006 | Loss: 6866.1989\n",
      "MSE: 4436.2729\n",
      "Epoch 00007 | Loss: 4464.9954\n",
      "Epoch 00008 | Loss: 3334.9351\n",
      "Epoch 00009 | Loss: 2987.4729\n",
      "Epoch 00010 | Loss: 2803.0264\n",
      "Epoch 00011 | Loss: 2307.8507\n",
      "MSE: 1744.7559\n",
      "Epoch 00012 | Loss: 2415.5899\n",
      "Epoch 00013 | Loss: 2314.9784\n",
      "Epoch 00014 | Loss: 2158.3478\n",
      "Epoch 00015 | Loss: 1983.9828\n",
      "Epoch 00016 | Loss: 1849.4295\n",
      "MSE: 1196.8413\n",
      "Epoch 00017 | Loss: 1608.6947\n",
      "Epoch 00018 | Loss: 922.9636\n",
      "Epoch 00019 | Loss: 2283.1604\n",
      "Epoch 00020 | Loss: 616.1161\n",
      "Epoch 00021 | Loss: 2086.6201\n",
      "MSE: 436.1094\n",
      "Epoch 00022 | Loss: 510.7840\n",
      "Epoch 00023 | Loss: 475.6896\n",
      "Epoch 00024 | Loss: 401.0600\n",
      "Epoch 00025 | Loss: 354.5045\n",
      "Epoch 00026 | Loss: 305.6034\n",
      "MSE: 215.6701\n",
      "Epoch 00027 | Loss: 256.3865\n",
      "Epoch 00028 | Loss: 228.8221\n",
      "Epoch 00029 | Loss: 188.7701\n",
      "Epoch 00030 | Loss: 157.7618\n",
      "Epoch 00031 | Loss: 110.8770\n",
      "MSE: 72.9610\n",
      "Epoch 00032 | Loss: 91.1606\n",
      "Epoch 00033 | Loss: 75.3563\n",
      "Epoch 00034 | Loss: 69.5944\n",
      "Epoch 00035 | Loss: 70.2605\n",
      "Epoch 00036 | Loss: 67.3941\n",
      "MSE: 46.6542\n",
      "Epoch 00037 | Loss: 65.0175\n",
      "Epoch 00038 | Loss: 63.8788\n",
      "Epoch 00039 | Loss: 61.3924\n",
      "Epoch 00040 | Loss: 61.4385\n",
      "Epoch 00041 | Loss: 58.3465\n",
      "MSE: 44.1444\n",
      "Epoch 00042 | Loss: 56.8325\n",
      "Epoch 00043 | Loss: 57.2364\n",
      "Epoch 00044 | Loss: 58.5500\n",
      "Epoch 00045 | Loss: 52.0292\n",
      "Epoch 00046 | Loss: 54.0677\n",
      "MSE: 42.3432\n",
      "Epoch 00047 | Loss: 52.2730\n",
      "Epoch 00048 | Loss: 53.0501\n",
      "Epoch 00049 | Loss: 51.4957\n",
      "Epoch 00050 | Loss: 51.3665\n",
      "Epoch 00051 | Loss: 48.5940\n",
      "MSE: 38.0402\n",
      "Epoch 00052 | Loss: 49.1086\n",
      "Epoch 00053 | Loss: 47.6283\n",
      "Epoch 00054 | Loss: 44.7513\n",
      "Epoch 00055 | Loss: 43.4577\n",
      "Epoch 00056 | Loss: 43.8432\n",
      "MSE: 34.3661\n",
      "Epoch 00057 | Loss: 45.3518\n",
      "Epoch 00058 | Loss: 44.5428\n",
      "Epoch 00059 | Loss: 42.3660\n",
      "Epoch 00060 | Loss: 40.0792\n",
      "Epoch 00061 | Loss: 39.5318\n",
      "MSE: 31.1664\n",
      "Epoch 00062 | Loss: 41.1335\n",
      "Epoch 00063 | Loss: 40.3214\n",
      "Epoch 00064 | Loss: 37.6247\n",
      "Epoch 00065 | Loss: 38.0821\n",
      "Epoch 00066 | Loss: 36.8229\n",
      "MSE: 27.5484\n",
      "Epoch 00067 | Loss: 35.0408\n",
      "Epoch 00068 | Loss: 35.7063\n",
      "Epoch 00069 | Loss: 34.2856\n",
      "Epoch 00070 | Loss: 33.3605\n",
      "Epoch 00071 | Loss: 32.4730\n",
      "MSE: 22.8582\n",
      "Epoch 00072 | Loss: 32.2407\n",
      "Epoch 00073 | Loss: 29.9240\n",
      "Epoch 00074 | Loss: 29.5921\n",
      "Epoch 00075 | Loss: 28.9384\n",
      "Epoch 00076 | Loss: 29.0427\n",
      "MSE: 17.6220\n",
      "Epoch 00077 | Loss: 29.8969\n",
      "Epoch 00078 | Loss: 26.1907\n",
      "Epoch 00079 | Loss: 25.6351\n",
      "Epoch 00080 | Loss: 26.5288\n",
      "Epoch 00081 | Loss: 24.5369\n",
      "MSE: 14.5127\n",
      "Epoch 00082 | Loss: 24.6336\n",
      "Epoch 00083 | Loss: 24.0089\n",
      "Epoch 00084 | Loss: 23.7472\n",
      "Epoch 00085 | Loss: 23.4683\n",
      "Epoch 00086 | Loss: 23.4291\n",
      "MSE: 13.3954\n",
      "Epoch 00087 | Loss: 21.5423\n",
      "Epoch 00088 | Loss: 21.2218\n",
      "Epoch 00089 | Loss: 21.6585\n",
      "Epoch 00090 | Loss: 21.4571\n",
      "Epoch 00091 | Loss: 21.2322\n",
      "MSE: 12.8722\n",
      "Epoch 00092 | Loss: 22.2218\n",
      "Epoch 00093 | Loss: 21.2214\n",
      "Epoch 00094 | Loss: 21.7562\n",
      "Epoch 00095 | Loss: 22.2354\n",
      "Epoch 00096 | Loss: 22.0803\n",
      "MSE: 12.7254\n",
      "Epoch 00097 | Loss: 21.3809\n",
      "Epoch 00098 | Loss: 22.3581\n",
      "Epoch 00099 | Loss: 22.2588\n",
      "Epoch 00100 | Loss: 21.2473\n",
      "Epoch 00101 | Loss: 20.3652\n",
      "MSE: 12.5024\n",
      "Epoch 00102 | Loss: 20.8445\n",
      "Epoch 00103 | Loss: 21.6886\n",
      "Epoch 00104 | Loss: 21.8863\n",
      "Epoch 00105 | Loss: 20.3251\n",
      "Epoch 00106 | Loss: 21.7130\n",
      "MSE: 12.4597\n",
      "Epoch 00107 | Loss: 21.5536\n",
      "Epoch 00108 | Loss: 19.7298\n",
      "Epoch 00109 | Loss: 21.0416\n",
      "Epoch 00110 | Loss: 20.6625\n",
      "Epoch 00111 | Loss: 20.3814\n",
      "MSE: 12.3169\n",
      "Epoch 00112 | Loss: 22.0230\n",
      "Epoch 00113 | Loss: 20.1772\n",
      "Epoch 00114 | Loss: 21.0945\n",
      "Epoch 00115 | Loss: 20.2189\n",
      "Epoch 00116 | Loss: 20.2512\n",
      "MSE: 12.0977\n",
      "Epoch 00117 | Loss: 21.2896\n",
      "Epoch 00118 | Loss: 20.6601\n",
      "Epoch 00119 | Loss: 20.4246\n",
      "Epoch 00120 | Loss: 19.3716\n",
      "Epoch 00121 | Loss: 20.7815\n",
      "MSE: 11.9903\n",
      "Epoch 00122 | Loss: 19.6567\n",
      "Epoch 00123 | Loss: 19.8083\n",
      "Epoch 00124 | Loss: 21.5523\n",
      "Epoch 00125 | Loss: 21.1018\n",
      "Epoch 00126 | Loss: 20.7661\n",
      "MSE: 12.3586\n",
      "Epoch 00127 | Loss: 19.8167\n",
      "Epoch 00128 | Loss: 20.1786\n",
      "Epoch 00129 | Loss: 20.3930\n",
      "Epoch 00130 | Loss: 20.3816\n",
      "Epoch 00131 | Loss: 20.4164\n",
      "MSE: 11.9889\n",
      "Epoch 00132 | Loss: 20.1025\n",
      "Epoch 00133 | Loss: 20.6111\n",
      "Epoch 00134 | Loss: 21.9919\n",
      "Epoch 00135 | Loss: 19.9251\n",
      "Epoch 00136 | Loss: 20.1507\n",
      "MSE: 11.9295\n",
      "Epoch 00137 | Loss: 20.6547\n",
      "Epoch 00138 | Loss: 20.2941\n",
      "Epoch 00139 | Loss: 21.1598\n",
      "Epoch 00140 | Loss: 19.1278\n",
      "Epoch 00141 | Loss: 18.9240\n",
      "MSE: 11.8387\n",
      "Epoch 00142 | Loss: 20.7335\n",
      "Epoch 00143 | Loss: 19.1995\n",
      "Epoch 00144 | Loss: 19.6334\n",
      "Epoch 00145 | Loss: 19.2199\n",
      "Epoch 00146 | Loss: 20.4667\n",
      "MSE: 11.7440\n",
      "Epoch 00147 | Loss: 21.2640\n",
      "Epoch 00148 | Loss: 19.4610\n",
      "Epoch 00149 | Loss: 19.5277\n",
      "Epoch 00150 | Loss: 20.6723\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"\\nDevice: \", device)\n",
    "\n",
    "\n",
    "num_epochs = 150\n",
    "graph_convolution_no_weights = GAT(\n",
    "    input_size=n_features,\n",
    "    hidden_size=32,\n",
    "    output_size=1,\n",
    "    num_layers=1,\n",
    "    heads=2).to(device)\n",
    "\n",
    "loss_fcn = nn.MSELoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(graph_convolution_no_weights.parameters(), lr=0.005)\n",
    "\n",
    "epoch_list, GCN_MSE = train(graph_convolution_no_weights, loss_fcn, device, optimizer, num_epochs, train_dataloader, val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Device:  cuda\n",
      "Epoch 00001 | Loss: 0.3448\n",
      "MSE: 0.3360\n",
      "Epoch 00002 | Loss: 0.3428\n",
      "Epoch 00003 | Loss: 0.2871\n",
      "Epoch 00004 | Loss: 0.2500\n",
      "Epoch 00005 | Loss: 0.2454\n",
      "Epoch 00006 | Loss: 0.2426\n",
      "MSE: 0.2436\n",
      "Epoch 00007 | Loss: 0.2414\n",
      "Epoch 00008 | Loss: 0.2410\n",
      "Epoch 00009 | Loss: 0.2409\n",
      "Epoch 00010 | Loss: 0.2408\n",
      "Epoch 00011 | Loss: 0.2408\n",
      "MSE: 0.2431\n",
      "Epoch 00012 | Loss: 0.2407\n",
      "Epoch 00013 | Loss: 0.2407\n",
      "Epoch 00014 | Loss: 0.2407\n",
      "Epoch 00015 | Loss: 0.2407\n",
      "Epoch 00016 | Loss: 0.2407\n",
      "MSE: 0.2432\n",
      "Epoch 00017 | Loss: 0.2407\n",
      "Epoch 00018 | Loss: 0.2407\n",
      "Epoch 00019 | Loss: 0.2407\n",
      "Epoch 00020 | Loss: 0.2407\n",
      "Epoch 00021 | Loss: 0.2407\n",
      "MSE: 0.2432\n",
      "Epoch 00022 | Loss: 0.2407\n",
      "Epoch 00023 | Loss: 0.2407\n",
      "Epoch 00024 | Loss: 0.2407\n",
      "Epoch 00025 | Loss: 0.2407\n",
      "Epoch 00026 | Loss: 0.2407\n",
      "MSE: 0.2432\n",
      "Epoch 00027 | Loss: 0.2407\n",
      "Epoch 00028 | Loss: 0.2407\n",
      "Epoch 00029 | Loss: 0.2407\n",
      "Epoch 00030 | Loss: 0.2407\n",
      "Epoch 00031 | Loss: 0.2407\n",
      "MSE: 0.2431\n",
      "Epoch 00032 | Loss: 0.2407\n",
      "Epoch 00033 | Loss: 0.2407\n",
      "Epoch 00034 | Loss: 0.2407\n",
      "Epoch 00035 | Loss: 0.2407\n",
      "Epoch 00036 | Loss: 0.2407\n",
      "MSE: 0.2431\n",
      "Epoch 00037 | Loss: 0.2407\n",
      "Epoch 00038 | Loss: 0.2407\n",
      "Epoch 00039 | Loss: 0.2407\n",
      "Epoch 00040 | Loss: 0.2407\n",
      "Epoch 00041 | Loss: 0.2407\n",
      "MSE: 0.2431\n",
      "Epoch 00042 | Loss: 0.2407\n",
      "Epoch 00043 | Loss: 0.2407\n",
      "Epoch 00044 | Loss: 0.2407\n",
      "Epoch 00045 | Loss: 0.2406\n",
      "Epoch 00046 | Loss: 0.2406\n",
      "MSE: 0.2430\n",
      "Epoch 00047 | Loss: 0.2406\n",
      "Epoch 00048 | Loss: 0.2406\n",
      "Epoch 00049 | Loss: 0.2406\n",
      "Epoch 00050 | Loss: 0.2406\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"\\nDevice: \", device)\n",
    "\n",
    "\n",
    "num_epochs = 50\n",
    "graph_convolution_no_weights = GCN(\n",
    "    input_size=n_features,\n",
    "    hidden_size=64,\n",
    "    output_size=1).to(device)\n",
    "\n",
    "loss_fcn = nn.MSELoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(graph_convolution_no_weights.parameters(), lr=0.005)\n",
    "\n",
    "epoch_list, GCN_MSE = train(graph_convolution_no_weights, loss_fcn, device, optimizer, num_epochs, train_dataloader, val_dataloader)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Â Add a threshold "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing domain: Academia\n",
      "Preparing domain: Applied Sciences\n",
      "Preparing domain: Education\n",
      "Preparing domain: Engineering\n",
      "Preparing domain: Humanities\n",
      "Preparing domain: Mathematics and Computing\n",
      "Preparing domain: Medicine and Health\n",
      "Preparing domain: Natural Sciences\n",
      "Preparing domain: Social Sciences\n",
      "Number of samples in the train dataset:  9\n",
      "Number of samples in the val dataset:  9\n",
      "Number of samples in the test dataset:  9\n",
      "Output of one sample from the train dataset:  Data(edge_index=[2, 3641], y=[359], x=[359, 8], edge_attr=[3641, 3], domain='Academia', train_mask=[359], val_mask=[359], test_mask=[359], mask=[359])\n",
      "Edge_index :\n",
      "tensor([[  0,   0,   0,  ..., 354, 355, 356],\n",
      "        [ 89, 183,   0,  ..., 354, 355, 356]])\n",
      "Number of features per node:  8\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1\n",
    "threshold = 15\n",
    "\n",
    "# train dataset\n",
    "train_dataset = PyGAcademicGraph(split=\"train\", setting=\"transductive\", sparcify_threshold=threshold)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "\n",
    "# val dataset\n",
    "val_dataset = PyGAcademicGraph(split=\"val\", setting=\"transductive\", sparcify_threshold=threshold)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "# test dataset\n",
    "test_dataset = PyGAcademicGraph(split=\"test\", setting=\"transductive\", sparcify_threshold=threshold)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "# number of features\n",
    "n_features = train_dataset[0].x.shape[1]\n",
    "\n",
    "print(\"Number of samples in the train dataset: \", len(train_dataset))\n",
    "print(\"Number of samples in the val dataset: \", len(test_dataset))\n",
    "print(\"Number of samples in the test dataset: \", len(test_dataset))\n",
    "print(\"Output of one sample from the train dataset: \", train_dataset[0])\n",
    "print(\"Edge_index :\")\n",
    "print(train_dataset[0].edge_index)\n",
    "print(\"Number of features per node: \", n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Device:  cuda\n",
      "Epoch 00001 | Loss: 0.1918\n",
      "MSE: 0.1688\n",
      "Epoch 00002 | Loss: 0.1739\n",
      "Epoch 00003 | Loss: 0.1677\n",
      "Epoch 00004 | Loss: 0.1599\n",
      "Epoch 00005 | Loss: 0.1561\n",
      "Epoch 00006 | Loss: 0.1555\n",
      "MSE: 0.1422\n",
      "Epoch 00007 | Loss: 0.1538\n",
      "Epoch 00008 | Loss: 0.1531\n",
      "Epoch 00009 | Loss: 0.1526\n",
      "Epoch 00010 | Loss: 0.1514\n",
      "Epoch 00011 | Loss: 0.1500\n",
      "MSE: 0.1385\n",
      "Epoch 00012 | Loss: 0.1484\n",
      "Epoch 00013 | Loss: 0.1449\n",
      "Epoch 00014 | Loss: 0.1391\n",
      "Epoch 00015 | Loss: 0.1062\n",
      "Epoch 00016 | Loss: 0.0995\n",
      "MSE: 0.0702\n",
      "Epoch 00017 | Loss: 0.0654\n",
      "Epoch 00018 | Loss: 0.1545\n",
      "Epoch 00019 | Loss: 0.1903\n",
      "Epoch 00020 | Loss: 0.1932\n",
      "Epoch 00021 | Loss: 0.1898\n",
      "MSE: 0.1956\n",
      "Epoch 00022 | Loss: 0.1825\n",
      "Epoch 00023 | Loss: 0.1689\n",
      "Epoch 00024 | Loss: 0.0972\n",
      "Epoch 00025 | Loss: 0.0589\n",
      "Epoch 00026 | Loss: 0.0553\n",
      "MSE: 0.0542\n",
      "Epoch 00027 | Loss: 0.0481\n",
      "Epoch 00028 | Loss: 0.0872\n",
      "Epoch 00029 | Loss: 0.0834\n",
      "Epoch 00030 | Loss: 0.0667\n",
      "Epoch 00031 | Loss: 0.1217\n",
      "MSE: 0.1130\n",
      "Epoch 00032 | Loss: 0.0596\n",
      "Epoch 00033 | Loss: 0.1424\n",
      "Epoch 00034 | Loss: 0.1684\n",
      "Epoch 00035 | Loss: 0.0965\n",
      "Epoch 00036 | Loss: 0.0674\n",
      "MSE: 0.0470\n",
      "Epoch 00037 | Loss: 0.0571\n",
      "Epoch 00038 | Loss: 0.0455\n",
      "Epoch 00039 | Loss: 0.1031\n",
      "Epoch 00040 | Loss: 0.0692\n",
      "Epoch 00041 | Loss: 0.0747\n",
      "MSE: 0.0314\n",
      "Epoch 00042 | Loss: 0.1359\n",
      "Epoch 00043 | Loss: 0.1707\n",
      "Epoch 00044 | Loss: 0.1686\n",
      "Epoch 00045 | Loss: 0.1351\n",
      "Epoch 00046 | Loss: 0.0961\n",
      "MSE: 0.1210\n",
      "Epoch 00047 | Loss: 0.1329\n",
      "Epoch 00048 | Loss: 0.1373\n",
      "Epoch 00049 | Loss: 0.1357\n",
      "Epoch 00050 | Loss: 0.1311\n",
      "Epoch 00051 | Loss: 0.1237\n",
      "MSE: 0.1144\n",
      "Epoch 00052 | Loss: 0.1137\n",
      "Epoch 00053 | Loss: 0.0954\n",
      "Epoch 00054 | Loss: 0.0632\n",
      "Epoch 00055 | Loss: 0.0469\n",
      "Epoch 00056 | Loss: 0.0479\n",
      "MSE: 0.0218\n",
      "Epoch 00057 | Loss: 0.0781\n",
      "Epoch 00058 | Loss: 0.1160\n",
      "Epoch 00059 | Loss: 0.1140\n",
      "Epoch 00060 | Loss: 0.0963\n",
      "Epoch 00061 | Loss: 0.0598\n",
      "MSE: 0.0281\n",
      "Epoch 00062 | Loss: 0.0844\n",
      "Epoch 00063 | Loss: 0.1226\n",
      "Epoch 00064 | Loss: 0.1246\n",
      "Epoch 00065 | Loss: 0.1172\n",
      "Epoch 00066 | Loss: 0.1068\n",
      "MSE: 0.0924\n",
      "Epoch 00067 | Loss: 0.0863\n",
      "Epoch 00068 | Loss: 0.0550\n",
      "Epoch 00069 | Loss: 0.0514\n",
      "Epoch 00070 | Loss: 0.0565\n",
      "Epoch 00071 | Loss: 0.0576\n",
      "MSE: 0.0393\n",
      "Epoch 00072 | Loss: 0.0624\n",
      "Epoch 00073 | Loss: 0.0529\n",
      "Epoch 00074 | Loss: 0.1104\n",
      "Epoch 00075 | Loss: 0.0575\n",
      "Epoch 00076 | Loss: 0.0568\n",
      "MSE: 0.0282\n",
      "Epoch 00077 | Loss: 0.1169\n",
      "Epoch 00078 | Loss: 0.1080\n",
      "Epoch 00079 | Loss: 0.0748\n",
      "Epoch 00080 | Loss: 0.0693\n",
      "Epoch 00081 | Loss: 0.0716\n",
      "MSE: 0.0570\n",
      "Epoch 00082 | Loss: 0.0752\n",
      "Epoch 00083 | Loss: 0.0680\n",
      "Epoch 00084 | Loss: 0.0831\n",
      "Epoch 00085 | Loss: 0.0455\n",
      "Epoch 00086 | Loss: 0.0553\n",
      "MSE: 0.0503\n",
      "Epoch 00087 | Loss: 0.0510\n",
      "Epoch 00088 | Loss: 0.0626\n",
      "Epoch 00089 | Loss: 0.0672\n",
      "Epoch 00090 | Loss: 0.0510\n",
      "Epoch 00091 | Loss: 0.0389\n",
      "MSE: 0.0604\n",
      "Epoch 00092 | Loss: 0.0656\n",
      "Epoch 00093 | Loss: 0.0447\n",
      "Epoch 00094 | Loss: 0.0451\n",
      "Epoch 00095 | Loss: 0.0613\n",
      "Epoch 00096 | Loss: 0.0732\n",
      "MSE: 0.0612\n",
      "Epoch 00097 | Loss: 0.0682\n",
      "Epoch 00098 | Loss: 0.0540\n",
      "Epoch 00099 | Loss: 0.0434\n",
      "Epoch 00100 | Loss: 0.0707\n",
      "Epoch 00101 | Loss: 0.0570\n",
      "MSE: 0.0405\n",
      "Epoch 00102 | Loss: 0.0439\n",
      "Epoch 00103 | Loss: 0.0634\n",
      "Epoch 00104 | Loss: 0.0464\n",
      "Epoch 00105 | Loss: 0.0524\n",
      "Epoch 00106 | Loss: 0.0484\n",
      "MSE: 0.0393\n",
      "Epoch 00107 | Loss: 0.0427\n",
      "Epoch 00108 | Loss: 0.0423\n",
      "Epoch 00109 | Loss: 0.0462\n",
      "Epoch 00110 | Loss: 0.0384\n",
      "Epoch 00111 | Loss: 0.0391\n",
      "MSE: 0.0495\n",
      "Epoch 00112 | Loss: 0.0479\n",
      "Epoch 00113 | Loss: 0.0473\n",
      "Epoch 00114 | Loss: 0.0520\n",
      "Epoch 00115 | Loss: 0.0561\n",
      "Epoch 00116 | Loss: 0.0611\n",
      "MSE: 0.0540\n",
      "Epoch 00117 | Loss: 0.0420\n",
      "Epoch 00118 | Loss: 0.0643\n",
      "Epoch 00119 | Loss: 0.0748\n",
      "Epoch 00120 | Loss: 0.0591\n",
      "Epoch 00121 | Loss: 0.0460\n",
      "MSE: 0.0591\n",
      "Epoch 00122 | Loss: 0.0638\n",
      "Epoch 00123 | Loss: 0.0443\n",
      "Epoch 00124 | Loss: 0.0405\n",
      "Epoch 00125 | Loss: 0.0509\n",
      "Epoch 00126 | Loss: 0.0367\n",
      "MSE: 0.0384\n",
      "Epoch 00127 | Loss: 0.0401\n",
      "Epoch 00128 | Loss: 0.0493\n",
      "Epoch 00129 | Loss: 0.0408\n",
      "Epoch 00130 | Loss: 0.0560\n",
      "Epoch 00131 | Loss: 0.0981\n",
      "MSE: 0.1034\n",
      "Epoch 00132 | Loss: 0.0518\n",
      "Epoch 00133 | Loss: 0.0387\n",
      "Epoch 00134 | Loss: 0.0601\n",
      "Epoch 00135 | Loss: 0.0666\n",
      "Epoch 00136 | Loss: 0.0471\n",
      "MSE: 0.0655\n",
      "Epoch 00137 | Loss: 0.0517\n",
      "Epoch 00138 | Loss: 0.0584\n",
      "Epoch 00139 | Loss: 0.0624\n",
      "Epoch 00140 | Loss: 0.0689\n",
      "Epoch 00141 | Loss: 0.0538\n",
      "MSE: 0.0343\n",
      "Epoch 00142 | Loss: 0.0589\n",
      "Epoch 00143 | Loss: 0.0554\n",
      "Epoch 00144 | Loss: 0.0474\n",
      "Epoch 00145 | Loss: 0.0508\n",
      "Epoch 00146 | Loss: 0.0372\n",
      "MSE: 0.0426\n",
      "Epoch 00147 | Loss: 0.0395\n",
      "Epoch 00148 | Loss: 0.0432\n",
      "Epoch 00149 | Loss: 0.0466\n",
      "Epoch 00150 | Loss: 0.0326\n",
      "Epoch 00151 | Loss: 0.0351\n",
      "MSE: 0.0556\n",
      "Epoch 00152 | Loss: 0.0535\n",
      "Epoch 00153 | Loss: 0.0405\n",
      "Epoch 00154 | Loss: 0.0529\n",
      "Epoch 00155 | Loss: 0.0516\n",
      "Epoch 00156 | Loss: 0.0495\n",
      "MSE: 0.0280\n",
      "Epoch 00157 | Loss: 0.0461\n",
      "Epoch 00158 | Loss: 0.0472\n",
      "Epoch 00159 | Loss: 0.0405\n",
      "Epoch 00160 | Loss: 0.0512\n",
      "Epoch 00161 | Loss: 0.0514\n",
      "MSE: 0.0478\n",
      "Epoch 00162 | Loss: 0.0508\n",
      "Epoch 00163 | Loss: 0.0517\n",
      "Epoch 00164 | Loss: 0.0506\n",
      "Epoch 00165 | Loss: 0.0443\n",
      "Epoch 00166 | Loss: 0.0486\n",
      "MSE: 0.0272\n",
      "Epoch 00167 | Loss: 0.0430\n",
      "Epoch 00168 | Loss: 0.0470\n",
      "Epoch 00169 | Loss: 0.0357\n",
      "Epoch 00170 | Loss: 0.0265\n",
      "Epoch 00171 | Loss: 0.0491\n",
      "MSE: 0.0538\n",
      "Epoch 00172 | Loss: 0.0524\n",
      "Epoch 00173 | Loss: 0.0529\n",
      "Epoch 00174 | Loss: 0.0630\n",
      "Epoch 00175 | Loss: 0.0791\n",
      "Epoch 00176 | Loss: 0.0311\n",
      "MSE: 0.0302\n",
      "Epoch 00177 | Loss: 0.0456\n",
      "Epoch 00178 | Loss: 0.0471\n",
      "Epoch 00179 | Loss: 0.0463\n",
      "Epoch 00180 | Loss: 0.0454\n",
      "Epoch 00181 | Loss: 0.0413\n",
      "MSE: 0.0493\n",
      "Epoch 00182 | Loss: 0.0475\n",
      "Epoch 00183 | Loss: 0.0566\n",
      "Epoch 00184 | Loss: 0.0653\n",
      "Epoch 00185 | Loss: 0.0644\n",
      "Epoch 00186 | Loss: 0.0618\n",
      "MSE: 0.0594\n",
      "Epoch 00187 | Loss: 0.0501\n",
      "Epoch 00188 | Loss: 0.0705\n",
      "Epoch 00189 | Loss: 0.0689\n",
      "Epoch 00190 | Loss: 0.0549\n",
      "Epoch 00191 | Loss: 0.0686\n",
      "MSE: 0.0780\n",
      "Epoch 00192 | Loss: 0.0873\n",
      "Epoch 00193 | Loss: 0.0524\n",
      "Epoch 00194 | Loss: 0.0718\n",
      "Epoch 00195 | Loss: 0.0953\n",
      "Epoch 00196 | Loss: 0.0728\n",
      "MSE: 0.0390\n",
      "Epoch 00197 | Loss: 0.0682\n",
      "Epoch 00198 | Loss: 0.0534\n",
      "Epoch 00199 | Loss: 0.0457\n",
      "Epoch 00200 | Loss: 0.0538\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"\\nDevice: \", device)\n",
    "\n",
    "\n",
    "num_epochs = 200\n",
    "graph_convolution_no_weights = MPGCN_Net(\n",
    "    in_channels=n_features,\n",
    "    hidden_channels=64,\n",
    "    out_channels=1,\n",
    "    ).to(device)\n",
    "\n",
    "loss_fcn = nn.MSELoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(graph_convolution_no_weights.parameters(), lr=0.005)\n",
    "\n",
    "epoch_list, GCN_MSE = train(graph_convolution_no_weights, loss_fcn, device, optimizer, num_epochs, train_dataloader, val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Device:  cuda\n",
      "Epoch 00001 | Loss: 0.2402\n",
      "MSE: 0.2408\n",
      "Epoch 00002 | Loss: 0.2402\n",
      "Epoch 00003 | Loss: 0.2402\n",
      "Epoch 00004 | Loss: 0.2402\n",
      "Epoch 00005 | Loss: 0.2402\n",
      "Epoch 00006 | Loss: 0.2402\n",
      "MSE: 0.2408\n",
      "Epoch 00007 | Loss: 0.2402\n",
      "Epoch 00008 | Loss: 0.2402\n",
      "Epoch 00009 | Loss: 0.2402\n",
      "Epoch 00010 | Loss: 0.2402\n",
      "Epoch 00011 | Loss: 0.2402\n",
      "MSE: 0.2408\n",
      "Epoch 00012 | Loss: 0.2402\n",
      "Epoch 00013 | Loss: 0.2402\n",
      "Epoch 00014 | Loss: 0.2402\n",
      "Epoch 00015 | Loss: 0.2402\n",
      "Epoch 00016 | Loss: 0.2402\n",
      "MSE: 0.2408\n",
      "Epoch 00017 | Loss: 0.2402\n",
      "Epoch 00018 | Loss: 0.2402\n",
      "Epoch 00019 | Loss: 0.2402\n",
      "Epoch 00020 | Loss: 0.2402\n",
      "Epoch 00021 | Loss: 0.2402\n",
      "MSE: 0.2408\n",
      "Epoch 00022 | Loss: 0.2402\n",
      "Epoch 00023 | Loss: 0.2402\n",
      "Epoch 00024 | Loss: 0.2402\n",
      "Epoch 00025 | Loss: 0.2402\n",
      "Epoch 00026 | Loss: 0.2402\n",
      "MSE: 0.2408\n",
      "Epoch 00027 | Loss: 0.2402\n",
      "Epoch 00028 | Loss: 0.2402\n",
      "Epoch 00029 | Loss: 0.2402\n",
      "Epoch 00030 | Loss: 0.2402\n",
      "Epoch 00031 | Loss: 0.2402\n",
      "MSE: 0.2408\n",
      "Epoch 00032 | Loss: 0.2402\n",
      "Epoch 00033 | Loss: 0.2402\n",
      "Epoch 00034 | Loss: 0.2402\n",
      "Epoch 00035 | Loss: 0.2402\n",
      "Epoch 00036 | Loss: 0.2402\n",
      "MSE: 0.2408\n",
      "Epoch 00037 | Loss: 0.2402\n",
      "Epoch 00038 | Loss: 0.2402\n",
      "Epoch 00039 | Loss: 0.2402\n",
      "Epoch 00040 | Loss: 0.2402\n",
      "Epoch 00041 | Loss: 0.2402\n",
      "MSE: 0.2408\n",
      "Epoch 00042 | Loss: 0.2402\n",
      "Epoch 00043 | Loss: 0.2402\n",
      "Epoch 00044 | Loss: 0.2402\n",
      "Epoch 00045 | Loss: 0.2402\n",
      "Epoch 00046 | Loss: 0.2402\n",
      "MSE: 0.2408\n",
      "Epoch 00047 | Loss: 0.2402\n",
      "Epoch 00048 | Loss: 0.2402\n",
      "Epoch 00049 | Loss: 0.2402\n",
      "Epoch 00050 | Loss: 0.2402\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"\\nDevice: \", device)\n",
    "\n",
    "\n",
    "num_epochs = 50\n",
    "graph_convolution_no_weights = GCN(\n",
    "    input_size=n_features,\n",
    "    hidden_size=64,\n",
    "    output_size=1).to(device)\n",
    "\n",
    "loss_fcn = nn.MSELoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(graph_convolution_no_weights.parameters(), lr=0.005)\n",
    "\n",
    "epoch_list, GCN_MSE = train(graph_convolution_no_weights, loss_fcn, device, optimizer, num_epochs, train_dataloader, val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'PyGAcademicGraph' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m threshold \u001b[39m=\u001b[39m \u001b[39m15\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[39m# train dataset\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m train_dataset \u001b[39m=\u001b[39m PyGAcademicGraph(split\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m, setting\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39minductive\u001b[39m\u001b[39m\"\u001b[39m, sparcify_threshold\u001b[39m=\u001b[39mthreshold)\n\u001b[1;32m      6\u001b[0m train_dataloader \u001b[39m=\u001b[39m DataLoader(train_dataset, batch_size\u001b[39m=\u001b[39mbatch_size)\n\u001b[1;32m      8\u001b[0m \u001b[39m# val dataset\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'PyGAcademicGraph' is not defined"
     ]
    }
   ],
   "source": [
    "batch_size = 1\n",
    "threshold = 15\n",
    "\n",
    "# train dataset\n",
    "train_dataset = PyGAcademicGraph(split=\"train\", setting=\"inductive\", sparcify_threshold=threshold)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "\n",
    "# val dataset\n",
    "val_dataset = PyGAcademicGraph(split=\"val\", setting=\"inductive\", sparcify_threshold=threshold)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "# test dataset\n",
    "test_dataset = PyGAcademicGraph(split=\"test\", setting=\"inductive\", sparcify_threshold=threshold)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "# number of features\n",
    "n_features = train_dataset[0].x.shape[1]\n",
    "\n",
    "print(\"Number of samples in the train dataset: \", len(train_dataset))\n",
    "print(\"Number of samples in the val dataset: \", len(test_dataset))\n",
    "print(\"Number of samples in the test dataset: \", len(test_dataset))\n",
    "print(\"Output of one sample from the train dataset: \", train_dataset[0])\n",
    "print(\"Edge_index :\")\n",
    "print(train_dataset[0].edge_index)\n",
    "print(\"Number of features per node: \", n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
